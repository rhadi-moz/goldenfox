
####  QWEN 2.5

# QWEN 0.5B
model:
  repo: 'Qwen/Qwen2.5-0.5B-Instruct-GGUF'
  model_name:  'qwen2.5-0.5b-instruct-q8_0.gguf'
  quantization: 'q8_0'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


## QWEN 1.5B
model:
  repo: 'Qwen/Qwen2.5-1.5B-Instruct-GGUF'
  model_name:  'qwen2.5-1.5b-instruct-q8_0.gguf'
  quantization: 'q8_0'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


### QWEN 3B Instruct
model:
  repo: 'Qwen/Qwen2.5-3B-Instruct-GGUF'
  model_name: 'qwen2.5-3b-instruct-q4_k_m.gguf'
  quantization: 'q4_k_m'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


# QWEN 7B
model:
  repo: 'QuantFactory/Qwen2.5-7B-GGUF'
  model_name:  'Qwen2.5-7B.Q4_K_M.gguf'
  quantization: 'q4_k_m'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True

## QWEN 14B
model:
  repo: 'QuantFactory/Qwen2.5-14B-GGUF'
  model_name:  'Qwen2.5-14B.Q4_K_M.gguf'
  quantization: 'q4_k_m.gguf'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True

## QWEN 32B
model:
  repo: 'bartowski/Qwen2.5-32B-Instruct-GGUF'
  model_name:  'Qwen2.5-32B-Instruct-Q2_K.gguf'
  quantization: 'q2_K.gguf'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True



# GEMMA 3

# 1B
model:
  repo: 'unsloth/gemma-3-1b-it-GGUF'
  model_name:  'gemma-3-1b-it-Q8_0.gguf'
  quantization: 'q8_0'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True

# 4B
model:
  repo: 'unsloth/gemma-3-4b-it-GGUF'
  model_name:  'gemma-3-4b-it-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


# 12 B
model:
  repo: 'unsloth/gemma-3-12b-it-GGUF'
  model_name:  'gemma-3-12b-it-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True



## 27B
model:
  repo: 'bartowski/google_gemma-3-27b-it-GGUF'
  model_name:  'google_gemma-3-27b-it-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


####### LLAMA 3.2 #######
# 1B
model:
  repo: 'bartowski/Llama-3.2-1B-Instruct-GGUF'
  model_name:  'Llama-3.2-1B-Instruct-Q6_K_L.gguf'
  quantization: 'Q6_K_L'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True



##### 3B #####
model:
  repo: 'bartowski/Llama-3.2-3B-Instruct-GGUF'
  model_name:  'Llama-3.2-3B-Instruct-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


### 3.1 8B ####
model:
  repo: 'bartowski/Meta-Llama-3-8B-Instruct-GGUF'
  model_name:  'Meta-Llama-3-8B-Instruct-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


### 3.2 11B VISION
model:
  repo: 'leafspark/Llama-3.2-11B-Vision-Instruct-GGUF'
  model_name:  'Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


  # OLMO2 13B
model:
  repo: 'allenai/OLMo-2-1124-13B-Instruct-GGUF'
  model_name:  'olmo-2-1124-13B-instruct-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True

# OLMO2 7B
model:
  repo: 'allenai/OLMo-2-1124-7B-Instruct-GGUF'
  model_name:  'olmo-2-1124-7B-instruct-Q6_K.gguf'
  quantization: 'Q6_K'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


# OLMoE
model:
  repo: 'allenai/OLMoE-1B-7B-0924-Instruct-GGUF'
  model_name:  'olmoe-1b-7b-0924-instruct-q6_k.gguf'
  quantization: 'Q6_K'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True



 # PHI 4 MINI
model:
  repo: 'unsloth/Phi-4-mini-instruct-GGUF'
  model_name:  'Phi-4-mini-instruct-Q4_K_M.gguf'
  quantization: 'Q4_K_M'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


## Phi 4 - 14B
model:
  repo: 'microsoft/phi-4-gguf'
  model_name:  'phi-4-q4.gguf'
  quantization: 'q4_0'
  temperature:  .2
  top_k:  40
  top_p:  .95
  max_tokens:  100
  min_p:  0.05
  n_ctx: 11352 # estimated from input data
  truncate: True


  ### MISTRAL ####
  ## 24B

  model:
    repo: 'bartowski/mistralai_Mistral-Small-3.1-24B-Instruct-2503-GGUF'
    model_name:  'mistralai_Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf'
    quantization: 'Q4_K_M'
    temperature:  .2
    top_k:  40
    top_p:  .95
    max_tokens:  100
    min_p:  0.05
    n_ctx: 11352 # estimated from input data
    truncate: True


### 7B
model:
    repo: 'MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF'
    model_name:  'Mistral-7B-Instruct-v0.3.Q4_K_M.gguf'
    quantization: 'Q4_K_M'
    temperature:  .2
    top_k:  40
    top_p:  .95
    max_tokens:  100
    min_p:  0.05
    n_ctx: 11352 # estimated from input data
    truncate: True

### SMOL LM2
### 135M
model:
    repo: 'MaziyarPanahi/SmolLM2-135M-Instruct-GGUF'
    model_name:  'SmolLM2-135M-Instruct.Q8_0.gguf'
    quantization: 'Q8_0'
    temperature:  .2
    top_k:  40
    top_p:  .95
    max_tokens:  100
    min_p:  0.05
    n_ctx: 11352 # estimated from input data
    truncate: True

## 360M
model:
    repo: 'HuggingFaceTB/SmolLM2-360M-Instruct-GGUF'
    model_name:  'smollm2-360m-instruct-q8_0.gguf'
    quantization: 'Q8_0'
    temperature:  .2
    top_k:  40
    top_p:  .95
    max_tokens:  100
    min_p:  0.05
    n_ctx: 11352 # estimated from input data
    truncate: True


### 1.7B
model:
    repo: 'bartowski/SmolLM2-1.7B-Instruct-GGUF'
    model_name:  'SmolLM2-1.7B-Instruct-Q8_0.gguf'
    quantization: 'Q8_0'
    temperature:  .2
    top_k:  40
    top_p:  .95
    max_tokens:  100
    min_p:  0.05
    n_ctx: 11352 # estimated from input data
    truncate: True

